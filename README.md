# OXEN-AI

Oxen AI adalah solusi cerdas untuk bisnis modernâ€”membantu mengotomatisasi proses, menganalisis data dengan cepat, dan mengambil keputusan lebih tepat. Lebih efisien, lebih akurat, dan jelas lebih pintar (tanpa minta lembur).

Aplikasi asisten bisnis berbasis AI yang menggunakan Next.js, Tailwind CSS, dan LLM dengan RAG (Retrieval-Augmented Generation) untuk menghasilkan jawaban dan dokumen bisnis dengan style Gen-Z yang engaging tapi tetap professional.

## ğŸ—ï¸ Arsitektur

```
User (Website)
    â†“
Next.js + Tailwind UI
    â†“
API Route (/api/chat, /api/letter)
    â†“
LLM + RAG
    â†“
Jawaban / Surat
```

## âœ¨ Fitur

- ğŸ’¬ **Chat Interface**: Berinteraksi dengan AI untuk mendapatkan jawaban tentang bisnis dan administrasi
- ğŸ“„ **Letter Generator**: Generate surat resmi dengan berbagai jenis format
- ğŸ” **RAG Integration**: Menggunakan Retrieval-Augmented Generation untuk memberikan jawaban yang lebih akurat
- ğŸ¨ **Modern UI**: Interface yang modern dan responsif menggunakan Tailwind CSS
- âš¡ **Streaming Response**: Respons cepat dengan streaming untuk pengalaman yang lebih baik
- ğŸ¯ **Business-Focused**: Fokus ke kebutuhan bisnis dan perusahaan
- ğŸ’¬ **Gen-Z Style**: Komunikasi dengan style Gen-Z yang engaging tapi tetap professional
- ğŸ“Š **Data Visualization**: Menampilkan grafik dan chart untuk data bisnis (line chart, bar chart, pie chart)
- ğŸ–¼ï¸ **Image Support**: Support untuk menampilkan gambar dalam response (extendable untuk image generation)

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+ 
- npm atau yarn
- LLM Provider API Key (OpenAI, Groq, Ollama, atau Hugging Face)

### Installation

1. Clone repository atau download project

2. Install dependencies:
```bash
npm install
```
**Catatan**: Beberapa error TypeScript mungkin muncul sebelum menjalankan `npm install`. Setelah instalasi selesai, error tersebut akan hilang karena semua type definitions sudah terinstall.

3. Buat file `.env.local` di root directory:

   **Untuk menggunakan Ollama dengan Llama (GRATIS, Lokal, Direkomendasikan):**
   ```env
   LLM_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama3
   ```
   Lihat [Quick Setup Llama](./QUICK_SETUP_LLAMA.md) untuk panduan lengkap.

   **Untuk menggunakan Groq (GRATIS - Cloud, CEPAT):**
   ```env
   LLM_PROVIDER=groq
   GROQ_API_KEY=your_groq_api_key_here
   ```
   Dapatkan API key gratis di: https://console.groq.com/

   **Untuk menggunakan OpenAI:**
   ```env
   LLM_PROVIDER=openai
   OPENAI_API_KEY=your_openai_api_key_here
   ```

   **ğŸ“– Panduan lengkap:**
   - [Quick Setup Llama](./QUICK_SETUP_LLAMA.md) - Setup Llama dengan Ollama
   - [Setup Llama Lengkap](./SETUP_LLAMA.md) - Semua opsi untuk Llama (Ollama + Hugging Face)

4. Jalankan development server:
```bash
npm run dev
```

5. Buka browser di [http://localhost:3000](http://localhost:3000)

## ğŸ“ Struktur Project

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/          # API endpoint untuk chat
â”‚   â”‚   â””â”€â”€ letter/        # API endpoint untuk generate surat
â”‚   â”œâ”€â”€ globals.css        # Global styles dengan Tailwind
â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â””â”€â”€ page.tsx           # Halaman utama
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatInterface.tsx  # Komponen chat UI
â”‚   â””â”€â”€ LetterGenerator.tsx # Komponen generator surat
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ providers.ts   # Multi-provider LLM support
â”‚       â””â”€â”€ rag-service.ts # Service untuk LLM dan RAG
â””â”€â”€ package.json
```

## ğŸ› ï¸ Teknologi

- **Next.js 14**: Framework React untuk production
- **TypeScript**: Type safety
- **Tailwind CSS**: Utility-first CSS framework
- **Multi-Provider LLM**: Support OpenAI, Groq, Ollama, Hugging Face, Gemini
- **RAG (Retrieval-Augmented Generation)**: Untuk jawaban yang lebih akurat
- **Streaming**: Untuk respons yang lebih cepat

## ğŸ“ API Endpoints

### POST `/api/chat`

Mengirim pesan chat dan mendapatkan jawaban dari AI dengan streaming support.

**Request Body:**
```json
{
  "message": "Pertanyaan Anda",
  "conversationHistory": [], // Optional
  "stream": true // Optional, default: true
}
```

**Response (Streaming):**
```
text/event-stream dengan chunks
```

**Response (Non-streaming):**
```json
{
  "success": true,
  "response": "Jawaban dari AI"
}
```

### POST `/api/letter`

Generate surat resmi.

**Request Body:**
```json
{
  "letterType": "resmi",
  "recipient": "Kepala Dinas...",
  "subject": "Perihal surat",
  "content": "Isi surat",
  "additionalContext": "Konteks tambahan" // Optional
}
```

**Response:**
```json
{
  "success": true,
  "letter": "Surat lengkap yang dihasilkan"
}
```

## ğŸ”§ Konfigurasi

### Menambahkan Dokumen ke RAG

Untuk meningkatkan kualitas jawaban, Anda dapat menambahkan dokumen ke vector store. Edit file `lib/llm/rag-service.ts` dan tambahkan dokumen di fungsi `initializeVectorStore()`.

### Mengubah Model LLM

Edit file `.env.local` dan ubah `LLM_PROVIDER` atau model spesifik untuk provider yang dipilih.

## ğŸš¢ Production Build

```bash
npm run build
npm start
```

## ğŸ“„ License

MIT

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
